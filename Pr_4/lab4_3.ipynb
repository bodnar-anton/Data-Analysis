{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "922f3af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a10baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90be2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a4a6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s. It was popularised in the 1960s with the release of Letraset sheets.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(summarizer(ARTICLE, max_length=60, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847acd31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6ce198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5c3839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = TFRobertaModel.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c86d051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLayer  multiple                 355359744 \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 355,359,744\n",
      "Trainable params: 355,359,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4dffe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91977583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(1, 17, 1024), dtype=float32, numpy=\n",
      "array([[[-0.06556174, -0.08852815, -0.02000626, ..., -0.11277131,\n",
      "          0.18897784,  0.12333396],\n",
      "        [ 0.02408867,  0.08807859, -0.2706828 , ...,  0.06151231,\n",
      "         -0.20081146, -0.03543307],\n",
      "        [ 0.08181281,  0.07480118, -0.24081755, ..., -0.1555765 ,\n",
      "          0.530776  ,  0.06391553],\n",
      "        ...,\n",
      "        [-0.05194158,  0.26970786, -0.525902  , ..., -0.01453426,\n",
      "          0.2245437 ,  0.20920593],\n",
      "        [-0.02740651,  0.20386069, -0.36803162, ..., -0.21442485,\n",
      "          0.03581668,  0.2231814 ],\n",
      "        [-0.02596354, -0.07665968,  0.02093653, ..., -0.1460183 ,\n",
      "          0.13287024,  0.04350654]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
      "array([[ 0.16108422,  0.6981167 ,  0.45465714, ...,  0.05722639,\n",
      "         0.33975062, -0.37429762]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
