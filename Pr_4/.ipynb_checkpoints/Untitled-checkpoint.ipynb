{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a426b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow.keras \n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout \n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D \n",
    "from keras.layers import LeakyReLU \n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D \n",
    "from keras.models import Sequential, Model \n",
    "from keras.optimizers import Adam,SGD \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import UpSampling2D, Conv2D \n",
    "from keras.models import Sequential, Model \n",
    "from keras.optimizers import Adam,SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97461fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the CIFAR10 data \n",
    "(x_train, y_train), (_, _) = tensorflow.keras.datasets.cifar100.load_data() \n",
    "\n",
    "save_path = './kaggle/working/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90360bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b63e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 10\n",
    "plt.figure(figsize=(20,8))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i])\n",
    "    ax.get_xaxis().set_visible(True)\n",
    "    ax.get_yaxis().set_visible(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (32, 32, 3) \n",
    "\n",
    "dimension = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(): \n",
    "\n",
    "\t\tmodel = Sequential() \n",
    "\n",
    "\t\t#Building the input layer \n",
    "\t\tmodel.add(Dense(128 * 8 * 8, activation=\"relu\", \n",
    "\t\t\t\t\t\tinput_dim=dimension)) \n",
    "\t\tmodel.add(Reshape((8, 8, 128))) \n",
    "\t\t\n",
    "\t\tmodel.add(UpSampling2D()) \n",
    "\t\t\n",
    "\t\tmodel.add(Conv2D(128, kernel_size=3, padding=\"same\")) \n",
    "\t\tmodel.add(BatchNormalization(momentum=0.78)) \n",
    "\t\tmodel.add(Activation(\"relu\")) \n",
    "\t\t\n",
    "\t\tmodel.add(UpSampling2D()) \n",
    "\t\t\n",
    "\t\tmodel.add(Conv2D(64, kernel_size=3, padding=\"same\")) \n",
    "\t\tmodel.add(BatchNormalization(momentum=0.78)) \n",
    "\t\tmodel.add(Activation(\"relu\")) \n",
    "\t\t\n",
    "\t\tmodel.add(Conv2D(3, kernel_size=3, padding=\"same\")) \n",
    "\t\tmodel.add(Activation(\"tanh\")) \n",
    "\n",
    "\t\tnoise = Input(shape=(dimension,)) \n",
    "\t\timage = model(noise) \n",
    "\n",
    "\t\treturn Model(noise, image) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(): \n",
    "\n",
    "\t\tmodel = Sequential() \n",
    "\n",
    "\t\tmodel.add(Conv2D(32, kernel_size=3, strides=2, \n",
    "\t\t\t\t\t\tinput_shape=image_shape, padding=\"same\")) \n",
    "\t\tmodel.add(LeakyReLU(alpha=0.2)) \n",
    "\t\tmodel.add(Dropout(0.25)) \n",
    "\t\t\n",
    "\t\tmodel.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\")) \n",
    "\t\tmodel.add(ZeroPadding2D(padding=((0,1),(0,1)))) \n",
    "\t\tmodel.add(BatchNormalization(momentum=0.82)) \n",
    "\t\tmodel.add(LeakyReLU(alpha=0.25)) \n",
    "\t\tmodel.add(Dropout(0.25)) \n",
    "\t\t\n",
    "\t\tmodel.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\")) \n",
    "\t\tmodel.add(BatchNormalization(momentum=0.82)) \n",
    "\t\tmodel.add(LeakyReLU(alpha=0.2)) \n",
    "\t\tmodel.add(Dropout(0.25)) \n",
    "\t\t\n",
    "\t\tmodel.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\")) \n",
    "\t\tmodel.add(BatchNormalization(momentum=0.8)) \n",
    "\t\tmodel.add(LeakyReLU(alpha=0.25)) \n",
    "\t\tmodel.add(Dropout(0.25)) \n",
    "\t\t\n",
    "\t\t#Building the output layer \n",
    "\t\tmodel.add(Flatten()) \n",
    "\t\tmodel.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "\t\timage = Input(shape=image_shape) \n",
    "\t\tvalidity = model(image) \n",
    "\n",
    "\t\treturn Model(image, validity) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(): \n",
    "        r, c = 5,5\n",
    "        noise = np.random.normal(0, 1, (r * c,dimension)) \n",
    "        generated_images = generator.predict(noise) \n",
    "\n",
    "        #Scaling the generated images \n",
    "        generated_images = 0.5 * generated_images + 0.5\n",
    "                    \n",
    "        fig, axs = plt.subplots(r, c) \n",
    "        count = 0\n",
    "        for i in range(r): \n",
    "            for j in range(c): \n",
    "                axs[i,j].imshow(generated_images[count, :,:,]) \n",
    "                axs[i,j].axis('off') \n",
    "                plt.savefig(f'{save_path}/gan-images_epoch-{epoch}.png')\n",
    "                count += 1\n",
    "        plt.show() \n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and compiling the discriminator \n",
    "discriminator = build_discriminator() \n",
    "discriminator.compile(loss='binary_crossentropy', \n",
    "\t\t\t\t\toptimizer=Adam(0.0002,0.5), \n",
    "\t\t\t\t\tmetrics=['accuracy']) \n",
    "\n",
    "#Making the Discriminator untrainable \n",
    "#so that the generator can learn from fixed gradient \n",
    "discriminator.trainable = True\n",
    "\n",
    "# Building the generator \n",
    "generator = build_generator() \n",
    "\n",
    "#Defining the input for the generator and generating the images \n",
    "dummy = Input(shape=(dimension,)) \n",
    "image = generator(dummy) \n",
    "\n",
    "\n",
    "#Checking the validity of the generated image \n",
    "valid = discriminator(image) \n",
    "\n",
    "#Defining the combined model of the Generator and the Discriminator \n",
    "combined_network = Model(dummy, valid) \n",
    "combined_network.compile(loss='binary_crossentropy', \n",
    "\t\t\t\t\t\toptimizer=Adam(0.0002,0.5)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "generator.summary()\n",
    "#plt.savefig('/kaggle/working/generator.png')\n",
    "plot_model(generator, to_file='./kaggle/working/generator.png', show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ebf599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "discriminator.summary()\n",
    "plot_model(discriminator, to_file='./kaggle/working/discriminator.png', show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=50000 \n",
    "batch_size=16\n",
    "display_after=100\n",
    "losses=[] \n",
    "\n",
    "#Normalizing the input \n",
    "x_train = (x_train / 127.5) - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfe072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Defining the Adversarial ground truths \n",
    "valid = np.ones((batch_size, 1)) \n",
    "\n",
    "#Adding some noise \n",
    "valid += 0.05 * np.random.random(valid.shape) \n",
    "fake = np.zeros((batch_size, 1)) \n",
    "fake += 0.05 * np.random.random(fake.shape) \n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "            \n",
    "            #Training the Discriminator \n",
    "              \n",
    "            #Sampling a random half of images \n",
    "            index = np.random.randint(0, x_train.shape[0], batch_size) \n",
    "            images = x_train[index] \n",
    "\n",
    "            #Sampling noise and generating a batch of new images \n",
    "            noise = np.random.normal(0, 1, (batch_size, dimension)) \n",
    "            generated_images = generator.predict(noise) \n",
    "\n",
    "\n",
    "            #Training the discriminator to detect more accurately \n",
    "            #whether a generated image is real or fake \n",
    "            discm_loss_real = discriminator.train_on_batch(images, valid) \n",
    "            discm_loss_fake = discriminator.train_on_batch(generated_images, fake) \n",
    "            discm_loss = 0.5 * np.add(discm_loss_real, discm_loss_fake) \n",
    "          \n",
    "            #Training the Generator \n",
    "            \n",
    "            #Training the generator to generate images \n",
    "            #which pass the authenticity test \n",
    "            genr_loss = combined_network.train_on_batch(noise, valid) \n",
    "            \n",
    "            #Tracking the progress\t\t\t\t \n",
    "            if epoch % display_after == 0: \n",
    "                print(epoch)\n",
    "                display_images()\n",
    "                #plt.savefig(f'{save_path}/gan-images_epoch-{epoch}.png')\n",
    "                #plt.show()\n",
    "            \n",
    "       \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "solid = x_train[:30] \n",
    "solid = 0.5 * solid + 0.5\n",
    "f, ax = plt.subplots(5,6, figsize=(15,8)) \n",
    "for i, image in enumerate(solid): \n",
    "\tax[i//6, i%6].imshow(image) \n",
    "\tax[i//6, i%6].axis('on') \n",
    "plt.savefig(\"./kaggle/working/originalimages.png\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5160249",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(size=(30, dimension)) \n",
    "generated_images = generator.predict(noise) \n",
    "generated_images = 0.5 * generated_images + 0.5\n",
    "f, ax = plt.subplots(5,6, figsize=(15,8)) \n",
    "for i, image in enumerate(generated_images): \n",
    "\tax[i//6, i%6].imshow(image) \n",
    "\tax[i//6, i%6].axis('on') \n",
    "plt.savefig(\"./kaggle/working/reconstructedImages.png\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256089b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "#TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf01364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"por-eng.zip\",\n",
    "    origin=\"file:///D:/vuz/ml/lr4/por-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"por-eng\" / \"por.txt\"\n",
    "#Португальська-Англійська\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, por, rubbish = line.split(\"\\t\")\n",
    "    por = \"[start] \" + por + \" [end]\"\n",
    "    text_pairs.append((eng, por))\n",
    "    \n",
    "#rubbish - третій зайвий рядок, де написана інформація про авторське право"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe107dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 80\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
    ")\n",
    "por_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_por_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "por_vectorization.adapt(train_por_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b002b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, por):\n",
    "    eng = eng_vectorization(eng)\n",
    "    por = por_vectorization(por)\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": por[:, :-1],}, por[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, por_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    por_texts = list(por_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, por_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30143cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 16\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669137a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 # This should be at least 30 for convergence\n",
    "print(train_ds.cardinality().numpy())\n",
    "print(val_ds.cardinality().numpy())\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "#from numba import cuda\n",
    "#@cuda.jit\n",
    "def run_cuda(train_ds, epochs, val_ds):\n",
    "    transformer.fit(train_ds, epochs=epochs, validation_data=val_ds).to(\"cuda\")\n",
    "\n",
    "\n",
    "run_cuda(train_ds, epochs, val_ds)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "por_vocab = por_vectorization.get_vocabulary()\n",
    "por_index_lookup = dict(zip(range(len(por_vocab)), por_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = por_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = por_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequence(input_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode_sequence(\"Hello, My name is Ivan! Nice to meet you. The weather is fine today.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192cb715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_1 = 'Hrlg !gsdghshdh lssteobmno'\n",
    "classifier(txt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cada704",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_2 = 'Can you please devide 200 by 10 for me? Thanks.'\n",
    "classifier(txt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from_pt=True\n",
    "model_name = \"gpt2\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff33f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([txt_1, txt_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5992547",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs_with_padding = tokenizer([txt_1, txt_2], padding = True, truncation = True, max_length = 256, return_tensors=\"tf\")\n",
    "inputs_with_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed534e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs_with_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bda18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "predictions = tf.nn.softmax(outputs[0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44696f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da76c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bonus task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383aebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = pipeline(\"text-classification\",model=\"bert-base-uncased\")\n",
    "pipe(\"Text example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d97139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
